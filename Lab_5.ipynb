{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8KMc5OKCrsYfIl1DZY2ee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yadukul/ML-Lab/blob/main/Lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xzJW0V4i_vX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d38b4ca-a11c-4197-802d-f7bcb1b18405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the trained perceptron:\n",
            "Input: [0 0], Output: 1\n",
            "Input: [0 1], Output: 0\n",
            "Input: [1 0], Output: 0\n",
            "Input: [1 1], Output: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the activation function (step function)\n",
        "def activation_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Define the XOR gate inputs\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "# Define the initial weights and bias\n",
        "W0 = 10\n",
        "W1 = 0.2\n",
        "W2 = -0.75\n",
        "bias = 0\n",
        "\n",
        "# Define the learning rate (Î±)\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Define the target outputs for XOR gate\n",
        "target_outputs = [0, 1, 1, 0]\n",
        "\n",
        "# Training the perceptron\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "        input_values = inputs[i]\n",
        "        target = target_outputs[i]\n",
        "\n",
        "        # Calculate the weighted sum\n",
        "        weighted_sum = W0 + W1 * input_values[0] + W2 * input_values[1] + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        output = activation_function(weighted_sum)\n",
        "\n",
        "        # Calculate the error\n",
        "        error = target - output\n",
        "        total_error += abs(error)\n",
        "\n",
        "        # Update weights and bias\n",
        "        W0 += learning_rate * error\n",
        "        W1 += learning_rate * error * input_values[0]\n",
        "        W2 += learning_rate * error * input_values[1]\n",
        "        bias += learning_rate * error\n",
        "\n",
        "    # Check for convergence\n",
        "    if total_error == 0:\n",
        "        print(f\"Converged after {epoch + 1} epochs.\")\n",
        "        break\n",
        "\n",
        "# Testing the trained perceptron\n",
        "print(\"Testing the trained perceptron:\")\n",
        "for input_values in inputs:\n",
        "    weighted_sum = W0 + W1 * input_values[0] + W2 * input_values[1] + bias\n",
        "    output = activation_function(weighted_sum)\n",
        "    print(f\"Input: {input_values}, Output: {output}\")\n"
      ]
    }
  ]
}